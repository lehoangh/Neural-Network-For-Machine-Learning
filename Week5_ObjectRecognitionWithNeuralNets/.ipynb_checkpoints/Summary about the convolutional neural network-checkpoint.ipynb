{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "[1. The general convolution neural network](#conv)  \n",
    "[2. Detailed AlexNet-SuperVision for ImageNet challenge](#AlexNet)  \n",
    "[3. Mathematical aspect of each layer of AlexNet](#mat)  \n",
    "[5.4. Convolutional neural networks for object recognition](#conv_net_obj_reg)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Week 5, we had studied about the convolution neural network through 2 specific examples:\n",
    "- handwritten digit recognition (LeNet5).\n",
    "- 3-D objects recognition (AlexNet - SuperVision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The general convolution neural network\n",
    "<a id = \"conv\"> </a>\n",
    "- CNNs are commonly made up of 3 layer types:\n",
    "    - Convolution & non-linearity (rectified linear units).\n",
    "    - Pooling: (typically) local maximum.\n",
    "    - Fully connected & non-linearity.\n",
    "- Common architecture:\n",
    "    1. Stack a few Conv-ReLU layers, followed by a Pool layer.\n",
    "    2. Repeat pattern until image has been reduced to a small representation.\n",
    "    3. Transition to one or more Fully connected(FC)-ReLU layers.\n",
    "    4. The last FC layer computes the output.\n",
    "        - Stack multiple stages of feature extractors, higher stages compute more global, more invariant features.\n",
    "        - Classification layer at the end.\n",
    "- For example, the architecture of LeNet5:\n",
    "![lenet5](images/lenet5.png)\n",
    "- Example of the architecture of AlexNet:\n",
    "![alexnet](images/alexnet.png)\n",
    "![alexnet_car](images/alexnet_car.png)\n",
    "It can be seen as the overview like that:\n",
    "![overview](images/overview.png)\n",
    "- The CNNs are supervised training of convolutional filters by back-propagation classification error with mini-batch gradient descent:  \n",
    "    **Loop**:\n",
    "    1. Sample a batch of training data (~100 images).\n",
    "    2. Forwards pass: compute loss (avg. over batch).\n",
    "    3. Backwards pass: compute gradient.\n",
    "    4. Update all parameters.  \n",
    "    **Note**: usually called \"stochastic gradient descent\" even though SGD has a batch size of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detailed AlexNet-SuperVision for ImageNet challenge\n",
    "<a id = \"AlexNet\"> </a>\n",
    "- This deep CNN has 7 hidden \"weight\" layers.\n",
    "- Entirely supervised.\n",
    "- More data = good.\n",
    "- Trained with stochastic gradient descent on two NVIDIA GPUs for about a week (5~6 days).\n",
    "![dataflow](images/dataflow.png)![dataflow_1](images/dataflow_1.png)\n",
    "![detailed_1](images/detailed_1.png)\n",
    "- The max pooling layers just follow first, second and fifth convolutional layers.\n",
    "![detailed_1](images/detailed_2.png)\n",
    "- The procedure of training the AlexNet's:\n",
    "    1. Error backpropagation.\n",
    "    2. Iteratively update weight matrix (or tensor) in each layer by a stochastic gradient descent approach.\n",
    "![training](images/training.png)\n",
    "- Some main points improving the performance of AlexNet that is done in ImageNet Challenge 2012:\n",
    "    1. Use regularization techniques:\n",
    "        - data augmentation techniques that consisted of image translations, horizontal (left-right) reflections and patch extractions (256x256) $\\rightarrow$ (224x224) (in order to get more training data).\n",
    "![data_aug](images/data_aug.png)\n",
    "        - dropout techniques in order to combat the problem of overfitting to the training data.\n",
    "    2. Trained the model using batch stochastic gradient descent, with specific values for momentum and weight decay.\n",
    "    3. Trained on two NVIDIA GPUs:\n",
    "![gpus](images/gpus.png)\n",
    "The GPUs communicate only in certain layers in layer 3 and 2 first FCs of the output's layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mathematical aspect of each layer of AlexNet\n",
    "<a id=\"mat\"> </a>\n",
    "![ovw](images/ovw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Convolution\n",
    "![convo](images/convo.png)\n",
    "- input: images 224x224x3 (3 channels are R-G-B)\n",
    "- convolution filter size: 11x11\n",
    "- stride: 4\n",
    "- output: 55x55x96\n",
    "We have the width of images is 224, the size of filters is 11 and the stride is 4 $\\rightarrow$ (224-11)/4+1 = 55 $\\rightarrow$ output volume has spatial area of 55x55.\n",
    "- depth (i.e. the number of filters): 96.\n",
    "$\\rightarrow$ output volume has size 55x55x96 = 290,400 neurons.\n",
    "- each neuron is connected to a region of size 11x11x3 in the input volume $\\rightarrow$ 363 weights + 1 bias.\n",
    "$\\rightarrow$ if each neuron has separate params, the first layer would need 290,400 * 364 > 100 million parameters.\n",
    "- **However**, natural images have the property of being **stationary**:\n",
    "    - the statistics in one part of the image are the same as of any other part.\n",
    "    - thus, we can use the same features at all locations.  \n",
    "    $\\rightarrow$ constrain the neurons in each depth slice (of 96 slices) to use the same params $\\rightarrow$ run the same filter or kernel over all receptive field windows (i.e. convolve the filter with the input image).\n",
    "    - AlexNet example:\n",
    "        - output volume has size 55x55x96 = 290,400 neurons.\n",
    "        - there are 96 depth slices (96 filters(, each with 55x55 neurons: all 55x55 have the same 11x11x3+1 = 364 params.  \n",
    "        $\\rightarrow$ only 96 * 364 = 34,944 params $\\rightarrow$ a dramatical reduction.\n",
    "![convo_exp](images/convo_exp.png)\n",
    "![convo_exp_1](images/convo_exp_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Max pooling (to downsampling)\n",
    "- The most common form is a max-pooling layer with a max filter of size 2x2 applied with a stride of 2 (size = 2x2, stride = 2).\n",
    "    - it downsamples every depth slice in the input by 2 along both width and height.\n",
    "    - the depth dimension remains unchanged.\n",
    "- Pooling reduces the spatial size of each depth slice in the output.\n",
    "$\\rightarrow$ fewer params in higher levels in the network.\n",
    "![pool](images/pool.png)\n",
    "![pool_demo](images/pool_demo.png)\n",
    "![pool_1](images/pool_1.png)\n",
    "- Role of pooling:\n",
    "    - invariance to small transformations.\n",
    "    - larger receptive fields (see more of input)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Local Response Normalization\n",
    "- no need to input normalization with ReLUs.\n",
    "- but still the following local normalization scheme helps generalizatioin.\n",
    "    - Let $a^i_{x,y}$ be the activity of a neuron computed by applying kernel i at position (x,y) and then applying the ReLU nonlinearity.\n",
    "    - compute the response-normalized activity, where the sum runs over n adjacent kernel maps at the same spatial position:\n",
    "![lrn](images/lrn.png)\n",
    "        - N: the total number of kernels in the layer.\n",
    "        - n: hyper-parameter, the number of adjacent kernel maps, n = 5.\n",
    "        - k: hyper-parameter, k = 2.\n",
    "        - $\\alpha$: hyper-parameter, $\\alpha = 10^{-4}$.\n",
    "        - $\\beta$: hyper-parameter, $\\beta = 0.75$.\n",
    "    - This aids generalization even though ReLU don't require it.\n",
    "    - is a sort of \"brightness normalization\".\n",
    "    - lateral inhibition: creating competition for big activities amongst neuron outputs computed using different kernels.\n",
    "![lrn_1](images/lrn_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Training\n",
    "- After convolution layer, training the weights and then applying non-linearity ReLU.\n",
    "![scheme](images/scheme.png)\n",
    "- Initialization:\n",
    "    - all feature extractors initialized at white Gaussian noise and learned from the data.\n",
    "    - $w$: zero-mean Gaussian with standard deviation 0,01.\n",
    "    - bias in neurons: 0 for 1st and 3rd layers and 1 for other layers.\n",
    "- Stochastic gradient descent learning:\n",
    "    - learning update rule with momentum (damping parameter) and learning rate $\\epsilon$:\n",
    "![update_rule](images/update_rule.png)\n",
    "        - $v$: momentum variable.\n",
    "        - $\\epsilon$: learning rate, 0,01 initially and reduced 3 times prior to termination.\n",
    "        - $<.>_{D_i}$: average over $i$-th batch $D_i$ (batch size = 128).\n",
    "    - Momentum: what is it?\n",
    "        - gradient descent finds only a local minima.\n",
    "            - is not a problem if $J(w)$ (the loss function) is small at a local minima. Of course, we do not wish to find $w$ w.r.t. $J(w) = 0$ due to overfitting.\n",
    "![good](images/good.png)\n",
    "            - but, is a problem if $J(w)$ is large at a local minimum $w$.\n",
    "![bad](images/bad.png)\n",
    "        - momentum: popular method to avoid local minima and also speed up descent in plateau regions.\n",
    "![momentum](images/momentum.png)\n",
    "![update](images/update.png)\n",
    "    - Weight decay: (L2 regularization)\n",
    "        - is also a regularization technique because the weights \"decay\" each iteration:\n",
    "![weight_decay](images/weight_decay.png)\n",
    "        - **Note**: typically, biases are excluded from regularization.\n",
    "- Backpropagation error:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
